{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_calendar = pd.read_csv(\"Calendar_with_cycled_days.csv\", index_col = 0)\n",
    "date_converter = dict(zip(store_calendar['d'], store_calendar.index))\n",
    "\n",
    "sell_prices = pd.read_csv(\"sell_prices_afcs2021.csv\", index_col=0)\n",
    "\n",
    "sample_submission = pd.read_csv(\"sample_submission_afcs2021.csv\", index_col=0)\n",
    "\n",
    "train_data = pd.read_csv(\"sales_train_validation_afcs2021.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"sales_test_validation_afcs2021.csv\", index_col=0)\n",
    "train_data = train_data.rename(columns=date_converter)\n",
    "test_data = test_data.rename(columns=date_converter)\n",
    "\n",
    "total_sales = train_data.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train = train_data.merge(test_data, left_index=True, right_index=True)\n",
    "big_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocess the train data\n",
    "ts_train_data = big_train.transpose()\n",
    "ts_train_data.index = pd.to_datetime(ts_train_data.index)\n",
    "products = list(ts_train_data.columns.values)\n",
    "originals = list(ts_train_data.columns.values)\n",
    "\n",
    "for i in products:\n",
    "    p_name = \"_\".join(i.split(\"_\")[:3])\n",
    "    ts_train_data = ts_train_data.rename(columns = {i : p_name})\n",
    "\n",
    "ts_train_data.index.name = 'date'\n",
    "ts_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for merging\n",
    "store_calendar = store_calendar.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = store_calendar['event_name_1']\n",
    "events_tf = np.zeros(len(events))\n",
    "\n",
    "for i in range(len(events)):\n",
    "    if isinstance(events[i], str):\n",
    "        events_tf[i] = 1\n",
    "\n",
    "events_tf = [int(item) for item in events_tf]\n",
    "        \n",
    "store_calendar['is_event'] = events_tf\n",
    "store_calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = store_calendar['event_name_1']\n",
    "unique_events = events.unique()[1:]\n",
    "unique_events\n",
    "\n",
    "for i in unique_events:\n",
    "    arr = np.zeros(len(events))\n",
    "    for j in range(len(events)):\n",
    "        if events[j] == i:\n",
    "            arr[j] = 1\n",
    "    store_calendar[i] = [int(item) for item in arr]\n",
    "\n",
    "store_calendar.columns.values[18:]\n",
    "# ts_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the calendar and sell_price dataframes\n",
    "new = pd.merge(sell_prices, store_calendar, on='wm_yr_wk')\n",
    "new = new.set_index('date')\n",
    "new.index = pd.to_datetime(new.index)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the sell price so it can be added to the VAR model (variables can't be constant)\n",
    "original = new['sell_price']\n",
    "noise = np.random.normal(.01, .001, len(new))\n",
    "new['sell_price'] = new['sell_price'] + noise\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_list = [['sin_wday', 'cos_wday', 'sell_price', 'is_event']]\n",
    "events = store_calendar.columns.values[18:]\n",
    "var_list.append(list(events))\n",
    "var_list = [item for sublist in var_list for item in sublist]\n",
    "var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in this cell from: https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=15\n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = ts_train_data[products[0]]\n",
    "df_sales = new[new['item_id'] == products[0]]\n",
    "dftest = pd.merge(df_sales, df_train, on=\"date\")[[products[0], 'is_event', 'sin_month', 'cos_month', 'sell_price']]\n",
    "print(dftest)\n",
    "\n",
    "grangers_causation_matrix(dftest, variables = dftest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in this cell from: https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\n",
    "\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "\n",
    "def adjust(val, length= 6): \n",
    "    return str(val).ljust(length)\n",
    "\n",
    "cointegration_test(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in this cell from: https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key,val in r[4].items():\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")\n",
    "        \n",
    "for name, column in df_differenced.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the forecasts\n",
    "def make_forecast(train_data, price, product_names, original_product_names):\n",
    "    submission = []\n",
    "    \n",
    "    for i in range(len(product_names)):\n",
    "        df_train = train_data[product_names[i]]\n",
    "        df_sales = price[price['item_id'] == product_names[i]]\n",
    "        df = pd.merge(df_sales, df_train, on=\"date\")[[product_names[i], 'sin_month', 'cos_month', 'sin_wday', 'cos_wday', 'is_event', 'sell_price']] #, 'SuperBowl', \n",
    "#                                                       'ValentinesDay', 'PresidentsDay', 'LentStart', 'LentWeek2', \n",
    "#                                                       'StPatricksDay', 'Purim End', 'OrthodoxEaster', 'Pesach End', \n",
    "#                                                       'Cinco De Mayo', \"Mother's day\", 'MemorialDay', 'NBAFinalsStart', \n",
    "#                                                       'NBAFinalsEnd', \"Father's day\", 'IndependenceDay', 'Ramadan starts', \n",
    "#                                                       'Eid al-Fitr', 'LaborDay', 'ColumbusDay', 'Halloween', 'EidAlAdha', \n",
    "#                                                       'VeteransDay', 'Thanksgiving', 'Christmas', 'Chanukah End', 'NewYear', \n",
    "#                                                       'OrthodoxChristmas', 'MartinLutherKingDay', 'Easter']]\n",
    "        \n",
    "\n",
    "#         Uncomment to make the data stationary\n",
    "#         df = df.diff().dropna()\n",
    "        \n",
    "        model = VAR(df)\n",
    "        model_fit = model.fit(maxlags=15, ic='bic')\n",
    "        \n",
    "#         Uncomment this to calculate the Durbin Watson statistic\n",
    "#         out = durbin_watson(model_fit.resid)\n",
    "#         for col, val in zip(df.columns, out):\n",
    "#             print(adjust(col), ':', round(val, 2))\n",
    "            \n",
    "        prediction = model_fit.forecast(model_fit.y, steps=28)\n",
    "        \n",
    "#         Uncomment to de-difference the predictions\n",
    "#         df_forecast = pd.DataFrame(prediction, columns=df.columns + '_2d')\n",
    "#         df_fc = df_forecast.copy()\n",
    "#         columns = df.columns\n",
    "#         for col in columns: \n",
    "#             df_fc[str(col)+'_forecast'] = df[col].iloc[-1] + df_fc[str(col)+'_2d'].cumsum()\n",
    "\n",
    "#         fc = df_fc[product_names[i]+'_forecast'].values.tolist()\n",
    "        \n",
    "        fc = prediction[:,0].tolist()\n",
    "        fc.insert(0, originals[i])\n",
    "        submission.append(fc)\n",
    "\n",
    "    return submission\n",
    " \n",
    "products = list(ts_train_data.columns.values)\n",
    "df = make_forecast(ts_train_data, new, products, originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecasts to a dataframe\n",
    "df = pd.DataFrame(df, columns=['id','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20','F21','F22','F23','F24','F25','F26','F27','F28'])\n",
    "df = df.set_index('id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE\n",
    "np.sqrt(mean_squared_error(df, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best RMSE for comparison\n",
    "best = df\n",
    "np.sqrt(mean_squared_error(best, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actuals vs the forecasts of the first product in the dataset\n",
    "plt.style.use('seaborn')\n",
    "df1 = best.iloc[1]\n",
    "df1.index = test_data.iloc[0].index\n",
    "plt.plot(df1)\n",
    "plt.plot(test_data.iloc[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actuals vs the forecasts of the average of all the products in the dataset\n",
    "summed = df.sum(axis=0)/823\n",
    "summed.index = test_data.iloc[0].index\n",
    "\n",
    "plt.plot(summed)\n",
    "plt.plot(test_data.sum(axis=0)/823)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CSV file\n",
    "sub = df.reset_index()\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
